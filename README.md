# ğŸ”ª Data Swiss Knife

> **The Ultimate Toolkit for Data Analysts, Data Engineers, Data Scientists, and Analytics Engineers**

A comprehensive, production-ready data analysis platform powered by **Streamlit** and **Ollama (gpt-oss-120b-cloud)**, featuring AI-assisted analysis, ETL pipelines, ML capabilities, and REST/WebSocket APIs.

[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/streamlit-1.28+-red.svg)](https://streamlit.io)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

![alt text](image.png)

## âœ¨ Features

### ğŸ¯ Core Capabilities

- **ğŸ“Š Interactive Data Explorer** - Filter, visualize, and analyze data with powerful interactive tools
- **ğŸ¤– AI Assistant** - Natural language queries powered by Ollama LLM (gpt-oss-120b-cloud)
- **ğŸ§¹ Data Cleaning** - Automated quality checks and cleaning pipelines (7 processors)
- **ğŸ“ˆ Advanced Analytics** - Statistical, time series, and ML-powered insights
- **âš™ï¸ ETL Pipelines** - Visual pipeline builder with 15+ transformations
- **ğŸ¨ Visualizations** - 15+ chart types (Plotly + Matplotlib)
- **ğŸ¤– Machine Learning** - AutoML with 11+ algorithms (Classification, Regression, Clustering)
- **ğŸ”— Multi-source Integration** - 9 data source types (CSV, Excel, SQL, NoSQL, APIs, Streaming)
- **ğŸŒ REST API** - FastAPI-powered REST endpoints
- **âš¡ WebSocket Streaming** - Real-time data streaming

### ğŸ—ï¸ Architecture & Design Patterns

- **Singleton** - LLM Client, Configuration
- **Strategy** - Data Loaders with Registry & AutoLoader
- **Factory** - Visualization Factory (Plotly + Matplotlib)
- **Chain of Responsibility** - Data Processing Pipelines
- **Observer** - Data Quality Monitoring
- **Template Method** - Analysis & Prompt Templates
- **Builder** - Query, Visualization & Pipeline Builders
- **Adapter** - LLM Provider Abstraction

---

## ğŸ“‹ Prerequisites

- **Python 3.9+**
- **[Ollama](https://ollama.ai/)** installed and running
- **8GB+ RAM** recommended
- **Docker** (optional, for containerized deployment)

---

## ğŸš€ Quick Start

### Method 1: Automated Setup âš¡

```bash
# Linux/Mac
chmod +x run.sh
./run.sh

# Windows
run.bat
```

The script will:

- âœ… Create virtual environment
- âœ… Install dependencies
- âœ… Check Ollama status
- âœ… Set up directories
- âœ… Launch application

---

### Method 2: Manual Setup ğŸ”§

#### 1. Install Ollama

```bash
# macOS/Linux
curl https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download

# Verify
ollama --version
```

#### 2. Pull LLM Model

```bash
# Start Ollama service
ollama serve

# In another terminal, pull the model
ollama pull gpt-oss-120b-cloud

# Alternative models
ollama pull llama3.1
ollama pull codellama
```

#### 3. Setup Project

```bash
# Clone repository
git clone https://github.com/yourusername/data-analysis-swiss-knife.git
cd data-analysis-swiss-knife

# Create virtual environment
python3 -m venv venv

# Activate
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

#### 4. Configure

```bash
# Copy environment template
cp .env.example .env

# Edit configuration (optional)
nano .env
```

#### 5. Run

```bash
# Launch Streamlit app
streamlit run streamlit_app/app.py

# Or using Make
make run
```

Access at: **http://localhost:8501**

---

### Method 3: Docker Setup ğŸ³

```bash
# Build and run
docker-compose up -d

# View logs
docker-compose logs -f app

# Stop
docker-compose down
```

---

## ğŸ“‚ Project Structure

```
data-analysis-swiss-knife/
â”‚
â”œâ”€â”€ ğŸ“„ Core Files
â”‚   â”œâ”€â”€ README.md âœ…
â”‚   â”œâ”€â”€ QUICKSTART.md âœ…
â”‚   â”œâ”€â”€ CONTRIBUTING.md âœ…
â”‚   â”œâ”€â”€ LICENSE (MIT) âœ…
â”‚   â”œâ”€â”€ requirements.txt âœ…
â”‚   â”œâ”€â”€ setup.py âœ…
â”‚   â”œâ”€â”€ Makefile âœ…
â”‚   â”œâ”€â”€ run.sh / run.bat âœ…
â”‚   â”œâ”€â”€ .env.example âœ…
â”‚   â”œâ”€â”€ Dockerfile âœ…
â”‚   â””â”€â”€ docker-compose.yml âœ…
â”‚
â”œâ”€â”€ ğŸ“ core/ - Core Library (4 files) âœ…
â”‚   â”œâ”€â”€ base.py              # 8 design patterns
â”‚   â”œâ”€â”€ config.py            # Singleton configuration
â”‚   â”œâ”€â”€ exceptions.py        # Custom exceptions
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ loaders/ - Data Loading (6 files) âœ…
â”‚   â”œâ”€â”€ base_loader.py       # Strategy + Registry + AutoLoader
â”‚   â”œâ”€â”€ csv_loader.py        # CSV, TSV, Excel, JSON, Parquet
â”‚   â”œâ”€â”€ database_loader.py   # PostgreSQL, MySQL, MongoDB, Redis, DuckDB
â”‚   â”œâ”€â”€ api_loader.py        # REST, GraphQL, Web Scraping
â”‚   â”œâ”€â”€ streaming_loader.py  # Kafka, WebSocket, MQTT âœ¨ NEW
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ processors/ - Data Processing (5 files) âœ…
â”‚   â”œâ”€â”€ cleaner.py          # 7 cleaning processors
â”‚   â”œâ”€â”€ transformer.py      # 4 feature transformers
â”‚   â”œâ”€â”€ aggregator.py       # 6 aggregators
â”‚   â”œâ”€â”€ validator.py        # 4 validators
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ analyzers/ - Analysis Engines (4 files) âœ…
â”‚   â”œâ”€â”€ statistical.py      # Statistical analysis
â”‚   â”œâ”€â”€ ml_insights.py      # ML-powered insights âœ¨ NEW
â”‚   â”œâ”€â”€ time_series.py      # Time series analysis âœ¨ NEW
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ visualizers/ - Visualizations (5 files) âœ…
â”‚   â”œâ”€â”€ base_viz.py         # Factory + Theme + Builder
â”‚   â”œâ”€â”€ static_charts.py    # 7 Matplotlib charts
â”‚   â”œâ”€â”€ interactive_plots.py # 8 Plotly charts
â”‚   â”œâ”€â”€ dashboard.py        # Dashboard builder
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ llm/ - AI Integration (3 files) âœ…
â”‚   â”œâ”€â”€ ollama_client.py    # Singleton LLM client
â”‚   â”œâ”€â”€ prompt_templates.py # 9 prompt templates
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ pipelines/ - ETL Pipelines (2 files) âœ…
â”‚   â”œâ”€â”€ etl_pipeline.py     # Pipeline + Builder
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ utils/ - Utilities (4 files) âœ…
â”‚   â”œâ”€â”€ helpers.py          # 15+ helper functions
â”‚   â”œâ”€â”€ decorators.py       # 12+ decorators
â”‚   â”œâ”€â”€ logging_config.py   # Logging setup
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ api/ - API Layer (3 files) âœ…
â”‚   â”œâ”€â”€ fastapi_app.py      # REST API âœ¨ NEW
â”‚   â”œâ”€â”€ websocket_handler.py # WebSocket streaming âœ¨ NEW
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ streamlit_app/ - Web UI (13 files) âœ…
â”‚   â”œâ”€â”€ app.py              # Main application
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ pages/ (6 pages)
â”‚   â”‚   â”œâ”€â”€ 1_ğŸ“Š_Data_Explorer.py
â”‚   â”‚   â”œâ”€â”€ 2_ğŸ§¹_Data_Cleaner.py
â”‚   â”‚   â”œâ”€â”€ 3_ğŸ“ˆ_Analytics.py
â”‚   â”‚   â”œâ”€â”€ 4_ğŸ¤–_AI_Assistant.py
â”‚   â”‚   â”œâ”€â”€ 5_âš™ï¸_ETL_Pipeline.py
â”‚   â”‚   â””â”€â”€ 6_ğŸ“‰_ML_Models.py
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ components/ (5 files)
â”‚       â”œâ”€â”€ sidebar.py
â”‚       â”œâ”€â”€ data_upload.py
â”‚       â”œâ”€â”€ query_builder.py
â”‚       â”œâ”€â”€ viz_selector.py
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ config/ (3 files) âœ…
â”‚   â”œâ”€â”€ settings.yaml
â”‚   â”œâ”€â”€ logging.yaml
â”‚   â””â”€â”€ llm_models.yaml
â”‚
â”œâ”€â”€ ğŸ“ .streamlit/ (1 file) âœ…
â”‚   â””â”€â”€ config.toml
â”‚
â”œâ”€â”€ ğŸ“ tests/ (2 files) âœ…
â”‚   â”œâ”€â”€ test_loaders.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ docs/ (2 files) âœ…
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â””â”€â”€ api_reference.md
â”‚
â”œâ”€â”€ ğŸ“ notebooks/ (3 files) âœ…
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â””â”€â”€ 03_model_training.ipynb
â”‚
â””â”€â”€ ğŸ“ data/ âœ…
    â”œâ”€â”€ raw/
    â”œâ”€â”€ processed/
    â””â”€â”€ results/
```

**Note:** All modules are at root level (no `src/` folder). Import as: `from loaders import CSVLoader`

---

## ğŸ¯ Usage Examples

### 1. Load Data

```python
from loaders import CSVLoader, AutoLoader

# Using specific loader
loader = CSVLoader()
df = loader.load("data/sales.csv")

# Or use AutoLoader
auto = AutoLoader()
df = auto.load("data/sales.csv")  # Detects format automatically
```

### 2. Clean Data

```python
from processors.cleaner import MissingValueHandler, DuplicateRemover
from core.base import DataContext

# Create context
context = DataContext(data=df, metadata={}, source="file", timestamp="now")

# Chain processors
missing_handler = MissingValueHandler(strategy='auto')
dup_remover = DuplicateRemover()

missing_handler.set_next(dup_remover)
cleaned_context = missing_handler.process(context)

cleaned_df = cleaned_context.data
```

### 3. AI-Powered Analysis

```python
from llm.ollama_client import get_ollama_client

client = get_ollama_client(model="gpt-oss-120b-cloud")

# Generate pandas code from natural language
code = client.nl_to_pandas(
    "Show me the top 10 products by sales",
    df_info={'columns': df.columns.tolist(), 'shape': df.shape}
)

# Get insights
insights = client.explain_data({'shape': df.shape, 'dtypes': df.dtypes.to_dict()})
```

### 4. Build ETL Pipeline

```python
from pipelines.etl_pipeline import PipelineBuilder
from processors.cleaner import MissingValueHandler
from processors.transformer import FeatureEngineer

pipeline = (PipelineBuilder("My Pipeline")
    .add_step(MissingValueHandler(strategy='mean'))
    .add_step(FeatureEngineer(operations=[{'type': 'polynomial', 'columns': ['age'], 'degree': 2}]))
    .build())

result = pipeline.execute(df)
```

### 5. Create Visualizations

```python
from visualizers.interactive_plots import PlotlyVisualizationFactory

factory = PlotlyVisualizationFactory()
scatter = factory.create_visualization("scatter")
fig = scatter.create(df, x="sales", y="profit", color="region")

# Or use viz selector in Streamlit
from streamlit_app.components.viz_selector import VizSelector

viz = VizSelector()
fig = viz.render(df)  # Interactive selection
```

### 6. Use REST API

```bash
# Upload data
curl -X POST "http://localhost:8000/upload" \
  -F "file=@data.csv"

# Analyze
curl -X POST "http://localhost:8000/analyze/data_0" \
  -H "Content-Type: application/json" \
  -d '{"operation": "summary", "parameters": {}}'

# Natural language query
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What are the top 10 customers by revenue?"}'
```

### 7. WebSocket Streaming

```python
import asyncio
import websockets

async def stream_data():
    uri = "ws://localhost:8000/ws"
    async with websockets.connect(uri) as websocket:
        # Subscribe to topic
        await websocket.send('{"type": "subscribe", "topic": "data_updates"}')

        # Receive updates
        while True:
            message = await websocket.recv()
            print(f"Received: {message}")

asyncio.run(stream_data())
```

---

## ğŸ› ï¸ Available Commands

### Using Make

```bash
make help          # Show all commands
make install       # Install dependencies
make dev           # Install dev dependencies
make test          # Run tests
make test-cov      # Run tests with coverage
make lint          # Run linters
make format        # Format code
make clean         # Clean cache files
make run           # Run Streamlit app
make docker-build  # Build Docker image
make docker-run    # Run in Docker
make setup-all     # Complete setup
```

### Manual Commands

```bash
# Development
pip install -r requirements.txt
pytest tests/
black .
flake8

# Run Streamlit
streamlit run streamlit_app/app.py

# Run FastAPI
python api/fastapi_app.py
# Or: uvicorn api.fastapi_app:app --reload

# Pull Ollama model
ollama pull gpt-oss-120b-cloud
```

---

## ğŸ“Š Complete Feature List

### Data Loading (9 Sources)

âœ… CSV, TSV, Excel, JSON, Parquet  
âœ… PostgreSQL, MySQL, SQLite, MongoDB, Redis, DuckDB  
âœ… REST API, GraphQL, Web Scraping  
âœ… Kafka, WebSocket, MQTT (Streaming)

### Data Processing (17 Processors)

âœ… 7 Cleaners: Missing values, duplicates, outliers, types, text, names, validation  
âœ… 4 Transformers: Feature engineering, scaling, encoding, aggregation  
âœ… 6 Aggregators: Standard, pivot, time series, rolling, custom, categorical

### Data Validation (4 Validators)

âœ… Schema, Data Quality, Business Rules, Referential Integrity

### Analysis (3 Analyzers)

âœ… Statistical Analysis (comprehensive)  
âœ… ML Insights (anomalies, clusters, patterns, feature importance)  
âœ… Time Series (trend, seasonality, stationarity, forecast, anomalies)

### Visualizations (15+ Types)

âœ… Plotly Interactive: Scatter, Line, Bar, Histogram, Box, Heatmap, Pie, Violin  
âœ… Matplotlib Static: Histogram, Scatter, Line, Bar, Box, Heatmap, Pie  
âœ… Dashboards with templates and export

### AI Features (9 Templates)

âœ… Code generation, Data explanation, Query translation  
âœ… Visualization suggestions, Code optimization  
âœ… SQL generation, Feature engineering  
âœ… Documentation, Error diagnosis

### Machine Learning (11+ Algorithms)

âœ… Classification: Logistic Regression, Random Forest, KNN, Naive Bayes, SVM  
âœ… Regression: Linear, Random Forest, Ridge, Lasso, KNN, SVR  
âœ… Clustering: K-Means, DBSCAN

### APIs

âœ… REST API with FastAPI (10+ endpoints)  
âœ… WebSocket for real-time streaming

---

## ğŸ”§ Configuration

### LLM Settings (`config/settings.yaml`)

```yaml
ollama:
  base_url: "http://localhost:11434"
  model: "gpt-oss-120b-cloud"
  temperature: 0.7
  max_tokens: 4096
```

### Database Connections (`.env`)

```bash
# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=analytics
POSTGRES_USER=analyst
POSTGRES_PASSWORD=yourpassword

# MongoDB
MONGO_URI=mongodb://localhost:27017/analytics

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# With coverage
pytest --cov=. --cov-report=html

# Specific test
pytest tests/test_loaders.py -v

# Using Make
make test
make test-cov
```

---

## ğŸ“ˆ Performance Tips

### For Large Datasets (1M+ rows)

```python
# 1. Use chunking
loader = CSVLoader()
for chunk in loader.load_chunked("large_file.csv", chunksize=10000):
    process(chunk)

# 2. Use DuckDB
from loaders.database_loader import DuckDBLoader
loader = DuckDBLoader()
df = loader.load("SELECT * FROM 'large_file.csv' WHERE condition")

# 3. Use Polars (faster than pandas)
import polars as pl
df = pl.read_csv("large_file.csv")
```

---

## ğŸš¢ Deployment

### Docker

```bash
# Build
docker build -t data-swiss-knife .

# Run
docker run -p 8501:8501 -p 8000:8000 data-swiss-knife
```

### Cloud Deployment

See `docs/deployment.md` for:

- AWS ECS/Fargate
- Google Cloud Run
- Azure Container Apps
- Heroku

---

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file.

---

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) - Local LLM runtime
- [Streamlit](https://streamlit.io/) - Web framework
- [Plotly](https://plotly.com/) - Interactive visualizations
- [FastAPI](https://fastapi.tiangolo.com/) - REST API framework
- [Pandas](https://pandas.pydata.org/) - Data manipulation

---

## ğŸ“§ Support

- ğŸ“– [Documentation](https://docs.example.com)
- ğŸ’¬ [Discord](https://discord.gg/example)
- ğŸ› [Issue Tracker](https://github.com/yourusername/data-analysis-swiss-knife/issues)
- ğŸ“§ Email: support@example.com

---

## ğŸ—ºï¸ Roadmap

- [ ] Advanced ML AutoML integration
- [ ] Real-time collaborative features
- [ ] Custom plugin system
- [ ] Mobile app
- [ ] Cloud data warehouse integration
- [ ] More LLM providers (OpenAI, Anthropic)
- [ ] Advanced security & governance

---

## ğŸ“Š Project Statistics

- **Total Files**: 65+
- **Lines of Code**: 22,000+
- **Design Patterns**: 8
- **Data Sources**: 9
- **Processors**: 17
- **Visualizations**: 15+
- **ML Algorithms**: 11+
- **Tests**: 15+

---

**Built with â¤ï¸ for the Data Community**

**Status**: âœ… Production Ready | **Version**: 1.0.0 | **Last Updated**: December 2025
